\section{Results and Discussion} \label{sec:results}

\input{fig/examplepanel}

\input{fig/recency-structure}

\subsection{Surface vs. Column} \label{sec:surface-vs-column}

\input{fig/col-vs-surf-summary}

\begin{itemize}
    \item surf steady worse than col steady
    \item surf tilted better than col tilted
\end{itemize}

From this point onwards, only surf algorithms!!!

\subsection{Steady vs. Tilted} \label{sec:steady-vs-tilted}
\begin{itemize}
    \item steady worse than tilted
    \item hybrid similar to tilted
\end{itemize}

\input{fig/steady-vs-tilted-summary}

\subsection{Why is Steady Bad? What Type of Error is Made?} \label{sec:error-analysis}
\begin{itemize}
    \item inner node loss from boxplot
    \item then, it's coming from recent nodes: node density vs. time histogram joyplot
    \item then, and that causes error: show error category (correct/wrong/unsure) vs. time stacked bar plot
\end{itemize}

From this point onwards, only surface-tilted (maybe also tilted-hybrid?) algorithm(s)!!!!

\subsection{Why are we seeing error? What can we do about it?} \label{sec:error-uncertainty}

mechanism of error: you have a lineage that diverges three ways between checkpoints (maybe make a figure of this); the two LEAST related happen to collide at the next checkpoint --- therefore, it looks like they're most related even though they're not.

sidebar: we can use differentia size to trade off between error and reconstruction accuracy; using 1 byte differentia essentially drives error to zero (but introduces uncertainty i.e., more polytomies in reconstruction)

\subsection{Does it Scale?} \label{sec:scaling}
\input{fig/scaling-summary}


\begin{itemize}
    \item show reconstruction error and inner node count vs. pop size (error does not increase with pop size at fixed sample size)
    \item show reconst error/inner node count vs. time (error does not increase with time at fixed sample size)
    \item show reconst error/inner node count vs. sample size (error increases with sample size)
    \item show reconst error/inner node count vs. bit size at large sample (we can reduce the error for large sample size by increasing bit size)
\end{itemize}

Note that inner node loss will likely be less of an issue for asynchronous generations on account of leaves being generationally dispersed.

\subsection{Reconstruction Quality of New Surface Algorithms}

\input{fig/col-vs-surf.tex}

Figure \ref{fig:col-vs-surf} compares reconstruction quality for surface algorithms against their corresponding column implementation.
We did this by using a simple simulation to generate phylogenies under different conditions with perfect tracking, then simulating heritage of hstrat annotations down the perfect tree and subsequent reconstruction.
We could then compare the reconstruction that would have been obtained under approximate tracking to the underlying ground truth.

To ensure generalizable results, we tested over a large number of evolutionary regimes, population sizes, instrumentation sizes, and instrumentation fingerprint sizes.
Each row in Figure \ref{fig:col-vs-surf} represents a distinct combination of surveyed conditions.

We took three measures of reconstruction quality.
The first measure, triplet distance, is a measure of accuracy --- it is the fraction of triplet tips that are correctly arranged in the reconstruction.
Whereas this metric considers polytomies as distinct from separate branching events, our second measure is a lax variant of triplet distance that does not penalize penalties introduced into the reconstruction (i.e., due to uncertainty about branching order) or over-resolution of true polytomies into more nodes in the reconstruction.
Finally, we also include inner node count, which provides a measure of the amount of detail achieved by reconstructions.
Higher inner node count indicates that fewer branching events are being lumped together into polytomies due to insufficient information to differentiate them.
This metric is only applicable to scenarios with fingerprint sizes larger than one bit (i.e., a byte), which are capable of generating non-bifurcating trees.

The data tell two clear stories:
\begin{enumerate}
\item surface tilted algorithms create higher-quality reconstructions than column-based tilted algorithms and
\item surface column algorithms create lower-quality reconstructions than column-based column algorithms.
\end{enumerate}

The surface tilted performs significantly best in 14 / 48 scenarios for triplet distance, 9 / 48 for lax triplet distance, and all scenarios where inner node count is applicable.
It performs significantly worse in no scenarios.

The surface steady performs significantly worse in 11 / 48 scenarios for triplet distance, in 1/48 scenarios for lax triplet distance, and 4/12 scenarios for inner node loss.
The surface steady performs significantly better than the column in no scenarios.

The performance advantage of surface-tilted is likely because the surface-based algorithms can make better use of available space --- every site is always in use to store a fingerprint.
In contrast, under the column-based approach, some fraction of the available space is typically unused because of difficulty predicting the order with which sites will need to be eliminated \textit{a priori} and thus eliminating them based on a heuristic (TODO rewrite this sentence).

The performance detriment of column-steady likely stems from precise control of the column approach to sequence eliminations.
The surface approach guarantees adherence to the fixed resolution qualities provided by the column approach, but the process of degrading from one spacing to the next double-width spacing is slightly more irregular on account of the additional consideration of being mapped onto the surface.
Unlike the tilted algorithm, the steady column algorithm does not have difficulty filling available space.

Recent work indicates that tilted policies should be preferred for better reconstruction accuracy in most cases, except where there are extreme factors promoting phylogenetic richness.
Thus, degraded reconstruction performance from the steady surface is not much of a concern because the steady policy is not expected to be used frequently in practice.
So, in addition to being capable of being implemented in a broader range of device contexts (not needing memory allocation or rich data structures) and being faster to calculate (discussed next), the surface-based approach can provide higher-quality phylogenetic reconstructions.