\section{Results and Discussion} \label{sec:results}

In this section, we report annotate-and-reconstruct experiments comparing phylogeny reconstruction quality obtained across possible hereditary stratigraphy approaches.
These experiments seek to establish an holistic, evidence-driven synopsis of each approach's suitability across experimental use cases.
The following section, ``A Practicioner's Guide to Hereditary Stratigraphy,'' then synthesizes findings to suggest guidelines for selecting appropriate methods to apply in practice.

This section delves into three primary aspects of hereditary stratigraphy methodology:
\begin{enumerate}
\item surface- versus column-based implementation,
\item tilted versus steady (versus hybrid) retention, and
\item bit- versus byte-sized differentiae.
\end{enumerate}

In a final set of experiments, we investigate how reconstruction quality fares with increasing phylogeny scale.
Scale-up of subsampled tip count and of underlying population size are both considered.
This question is crucial to application of hereditary stratigraphy for very large simulation use cases, in assessing the extent, if at all, annotation size would need to be boosted with increased experimental scale.

\subsection{Surface vs. Column Implementaiton} \label{sec:surface-vs-column}

\input{fig/col-vs-surf-summary}

Recall that surface- and column-based annotation implementations differ in how differentia are organized within a hereditary stratigraphy annotation.
Surface-based implementation takes a lower-level approach that simplifies acessioning of new differentia and ensure full use of available space, but sacrifices some control over the precise temporal distribution of retained differentiae.
Both implementations support tilted and steady retention policies.

Figure \ref{fig:col-vs-surf-summary} compares reconstruction quality for surface algorithms against their corresponding column implementation.
Outcomes differ notably between tilted and steady retention.

Under tilted retention, triplet distance (a measure of reconstruction accuracy) is equivalent or improved (in 14 / 48 scenarios) with surface-based implementation.
Inner node loss (a measure of reconstruction precision) improves in some scenarios and worsens in others.
Notably, shown in Supplementary Figure \ref{fig:col-vs-surf}, inner node loss improvement is seen in all treatments with byte-width differentiae, which are mechanistically more prone to create artifactual unresolved polytomies that drive inner node loss.
In sum, reconstruction quality of surface-based tilted retention can be considered as equivalent or superior across the board to that of column-based implementation.

In contrast, under steady retention, surface-based implementation achieves worse triplet distance reconstruction in 11 / 48 scenarios and better triplet distance in no scenario.
Similarly, inner node loss is worse in in 23 / 48 scenarios, including 4 / 12 byte-differentiae scenarios, and better in no scenario.
So, reconstruction quality of surface-based steady retention is equivalent or inferior to column-based implementation.

Why does the surface-based approach benefit reconstruction under one retention policy but not the other?
Compared to column-implementation, surface-implementation allows the tilted algorithm to retain more differentia within available annotation space.
Whereas the write-only design of the surface-based approach guarantees full use of buffer space, the column-based tilted retention holds space in reserve on account of discretization effects in tuning retention density.
In contrast, column-based steady retention makes full use of available space, giving the surface-based approach no advantage in this regard.
Although both approaches prune differentiae through comparable strided decimation procedures, the column implementation more systematically drops decimated differentiae from back to front.
This process ends up preserving more recent differentiae which, as we will see in the next set of experiments comparing steady and tilted retention, tends to benefit reconstruction quality.

Although surface-based implementation involves a trade-off between performance reconstruction quality in the steady case, it is notable that surface-based implementation provides uncompromised improvement in both runtime performance and data quality.
For greater detail of reconstruction quality outcomes broken down by treatment condition, see Supplementary Figure \ref{fig:col-vs-surf}.
Note that in the next set of experiments, which compares steady versus tilted retention, in the interest of even footing we report results for each retention policy using its best performing implementation (i.e., column vs. surface) as established here.

% We did this by using a simple simulation to generate phylogenies under different conditions with perfect tracking, then simulating heritage of hstrat annotations down the perfect tree and subsequent reconstruction.
% We could then compare the reconstruction that would have been obtained under approximate tracking to the underlying ground truth.

% To ensure generalizable results, we tested over a large number of evolutionary regimes, population sizes, instrumentation sizes, and instrumentation fingerprint sizes.

% We took three measures of reconstruction quality.
% The first measure, triplet distance, is a measure of accuracy --- it is the fraction of triplet tips that are correctly arranged in the reconstruction.
% Whereas this metric considers polytomies as distinct from separate branching events, our second measure is a lax variant of triplet distance that does not penalize penalties introduced into the reconstruction (i.e., due to uncertainty about branching order) or over-resolution of true polytomies into more nodes in the reconstruction.
% Finally, we also include inner node count, which provides a measure of the amount of precision achieved by reconstructions.
% Higher inner node count indicates that fewer branching events are being lumped together into polytomies due to insufficient information to differentiate them.
% This metric is only applicable to scenarios with fingerprint sizes larger than one bit (i.e., a byte), which are capable of generating non-bifurcating trees.

% The data tell two clear stories:
% \begin{enumerate}
% \item surface tilted algorithms create higher-quality reconstructions than column-based tilted algorithms and
% \item surface column algorithms create lower-quality reconstructions than column-based column algorithms.
% \end{enumerate}

\subsection{Steady vs. Tilted Retention} \label{sec:steady-vs-tilted}

\begin{itemize}
    \item steady worse than tilted
    \item hybrid similar to tilted
\end{itemize}

\input{fig/steady-vs-tilted-summary}

\subsection{Why is Steady Bad? What Type of Error is Made?} \label{sec:error-analysis}
\begin{itemize}
    \item inner node loss from boxplot
    \item then, it's coming from recent nodes: node density vs. time histogram joyplot
    \item then, and that causes error: show error category (correct/wrong/unsure) vs. time stacked bar plot
\end{itemize}

From this point onwards, only surface-tilted (maybe also tilted-hybrid?) algorithm(s)!!!!

\subsection{Why are we seeing error? What can we do about it?} \label{sec:error-uncertainty}

mechanism of error: you have a lineage that diverges three ways between checkpoints (maybe make a figure of this); the two LEAST related happen to collide at the next checkpoint --- therefore, it looks like they're most related even though they're not.

sidebar: we can use differentia size to trade off between error and reconstruction accuracy; using 1 byte differentia essentially drives error to zero (but introduces uncertainty i.e., more polytomies in reconstruction)

\subsection{Differentia Width} \label{sec:bit-vs-byte}

\input{fig/bit-vs-byte-summary}

\subsection{Does it Scale?} \label{sec:scaling}
\input{fig/scaling-summary}


\begin{itemize}
    \item show reconstruction error and inner node count vs. pop size (error does not increase with pop size at fixed sample size)
    \item show reconst error/inner node count vs. time (error does not increase with time at fixed sample size)
    \item show reconst error/inner node count vs. sample size (error increases with sample size)
    \item show reconst error/inner node count vs. bit size at large sample (we can reduce the error for large sample size by increasing bit size)
\end{itemize}

Note that inner node loss will likely be less of an issue for asynchronous generations on account of leaves being generationally dispersed.
