\section{Results and Discussion} \label{sec:results}

In this section, we report annotate-and-reconstruct experiments comparing phylogeny reconstruction quality obtained across possible hereditary stratigraphy approaches.
These experiments seek to establish an holistic, evidence-driven synopsis of each approach's suitability across experimental use cases.
The following section, ``A Practicioner's Guide to Hereditary Stratigraphy,'' then synthesizes findings to suggest guidelines for selecting appropriate methods to apply in practice.

This section delves into three primary aspects of hereditary stratigraphy methodology:
\begin{enumerate}
\item surface- versus column-based implementation,
\item tilted versus steady (versus hybrid) retention, and
\item bit- versus byte-sized differentiae.
\end{enumerate}

In a final set of experiments, we investigate how reconstruction quality fares with increasing phylogeny scale.
Scale-up of subsampled tip count and of underlying population size are both considered.
This question is crucial to application of hereditary stratigraphy for very large simulation use cases, in assessing the extent, if at all, annotation size would need to be boosted with increased experimental scale.

\subsection{Surface vs. Column Implementaiton} \label{sec:surface-vs-column}

\input{fig/col-vs-surf-summary}

Recall that surface- and column-based annotation implementations differ in how differentia are organized within a hereditary stratigraphy annotation.
Surface-based implementation takes a lower-level approach that simplifies acessioning of new differentia and ensure full use of available space, but sacrifices some control over the precise temporal distribution of retained differentiae.
Both implementations support tilted and steady retention policies.

Figure \ref{fig:col-vs-surf-summary} compares reconstruction quality for surface algorithms against their corresponding column implementation.
Outcomes differ notably between tilted and steady retention.

Under tilted retention, triplet distance (a measure of reconstruction accuracy) is equivalent or improved (in 14 / 48 scenarios) with surface-based implementation.
Inner node loss (a measure of reconstruction precision) improves in some scenarios and worsens in others.
Notably, shown in Supplementary Figure \ref{fig:col-vs-surf}, inner node loss improvement is seen in all treatments with byte-width differentiae, which are mechanistically more prone to create artifactual unresolved polytomies that drive inner node loss.
In sum, reconstruction quality of surface-based tilted retention can be considered as equivalent or superior across the board to that of column-based implementation.

In contrast, under steady retention, surface-based implementation achieves worse triplet distance reconstruction in 11 / 48 scenarios and better triplet distance in no scenario.
Similarly, inner node loss is worse in in 23 / 48 scenarios, including 4 / 12 byte-differentiae scenarios, and better in no scenario.
So, reconstruction quality of surface-based steady retention is equivalent or inferior to column-based implementation.

Why does the surface-based approach benefit reconstruction under one retention policy but not the other?
Compared to column-implementation, surface-implementation allows the tilted algorithm to retain more differentia within available annotation space.
Whereas the write-only design of the surface-based approach guarantees full use of buffer space, the column-based tilted retention holds space in reserve on account of discretization effects in tuning retention density.
In contrast, column-based steady retention makes full use of available space, giving the surface-based approach no advantage in this regard.
Although both approaches prune differentiae through comparable strided decimation procedures, the column implementation more systematically drops decimated differentiae from back to front.
This process ends up preserving more recent differentiae which, as we will see in the next set of experiments comparing steady and tilted retention, tends to benefit reconstruction quality.

Although surface-based implementation involves a trade-off between performance reconstruction quality in the steady case, it is notable that surface-based implementation provides uncompromised improvement in both runtime performance and data quality.
For greater detail of reconstruction quality outcomes broken down by treatment condition, see Supplementary Figure \ref{fig:col-vs-surf}.
Note that in the next set of experiments, which compares steady versus tilted retention, in the interest of even footing we report results for each retention policy using its best performing implementation (i.e., column vs. surface) as established here.

% We did this by using a simple simulation to generate phylogenies under different conditions with perfect tracking, then simulating heritage of hstrat annotations down the perfect tree and subsequent reconstruction.
% We could then compare the reconstruction that would have been obtained under approximate tracking to the underlying ground truth.

% To ensure generalizable results, we tested over a large number of evolutionary regimes, population sizes, instrumentation sizes, and instrumentation fingerprint sizes.

% We took three measures of reconstruction quality.
% The first measure, triplet distance, is a measure of accuracy --- it is the fraction of triplet tips that are correctly arranged in the reconstruction.
% Whereas this metric considers polytomies as distinct from separate branching events, our second measure is a lax variant of triplet distance that does not penalize penalties introduced into the reconstruction (i.e., due to uncertainty about branching order) or over-resolution of true polytomies into more nodes in the reconstruction.
% Finally, we also include inner node count, which provides a measure of the amount of precision achieved by reconstructions.
% Higher inner node count indicates that fewer branching events are being lumped together into polytomies due to insufficient information to differentiate them.
% This metric is only applicable to scenarios with fingerprint sizes larger than one bit (i.e., a byte), which are capable of generating non-bifurcating trees.

% The data tell two clear stories:
% \begin{enumerate}
% \item surface tilted algorithms create higher-quality reconstructions than column-based tilted algorithms and
% \item surface column algorithms create lower-quality reconstructions than column-based column algorithms.
% \end{enumerate}

\subsection{Steady vs. Tilted Retention} \label{sec:steady-vs-tilted}

\input{fig/steady-vs-tilted-summary}

Steady and tilted retention differ the composition of differentia checkpoint records maintained within hereditary stratigraphy annotations.
Recall that steady policy spaces retained differentia evenly across history, while tilted policy retains more of more recent differentia.
The question of which gives higher quality reconstruction, boils down to where precision in discerning the timing of branching events is most useful to resolving evolutionary history.
In the interest of even footing, we report results for each retention policy using its best performing implementation, as established above: steady policy uses column implementation and tilted policy uses surface implementation.
We also consider a hybrid policy, which splits surface buffer space evenly between steady and tilted retention.

Figure \ref{fig:steady-vs-tilted-summary} overviews how reconstruction quality differs by retention policy across use case scenarios.
Across the board, steady policy yields phylogenetic reconstruction with heaviest inner node loss.
In contrast, tilted policy exibits among the lowest inner node loss in all but six surveyed scenarios --- including all byte-differentia trials (Supplementary Figure \ref{fig:steady-vs-tilted}).
The hybrid policy has lower inner node loss in those six scenarios, which all involve evolutionary conditions with high phylgoenetic richness.

Steady policy also consistently produces the worst triplet distance error in lower-phylogenetic-richness plain and mild structure evolutionary scenarios.
However, in scenarios with high phylogenetic richness, triplet distance error under steady retention fare better.
With rich ecological/spatial structure, triplet distance reconstruction error is largely indistinguishable among retention policies.
Further, steady policy triplet error actually significantly outperforms tilted policy in several instances under drift conditions.
In nearly all of these instances, though, hybrid policy triplet error performs comparably to steady retention.
In scenarios with plain and mild structure scenarios, with low phylogenetic richness, hybrid triplet error is comparable to tilted error in 9 / 24 scenarios and outperformed in 15 / 24 scenarios.

Across the inner node loss and triplet error quality measures, tilted retention frequently performs best and steady retention frequently performs worst.
However, tilted retention has worse triplet error in some scenarios with high phylogenetic richness.
Hybrid retention performs more consistently across evolutionary scenarios.
It exhibits consistently intermediate levels of inner node loss that, in absolute terms, tend to not be far off from tilted retention.
Triplet error for hybrid retention is often comparable to the better-performing of steady and tilted retention, or at least intermediate between them.
For greater detail of steady vs. tilted reconstruction quality outcomes broken down by treatment condition, see Supplementary Figure \ref{fig:col-vs-surf}.

\input{fig/recency-structure}

Differences in retention of recent differentia explains the substantial advantage of tilted policy in use cases with low phylogenetic richness.
In such scenarios, frequent selective sweeps concentrate phylogenetic history over very recent history, meaning that lineage branching events giving rise to a contemporary extant population occured over a relatively short period of time.
Discerning these events, therefore, requires densely packed differentia checkpoints over recent history --- otherwise, in the worst case, taxa would jump from all sharing the same lineage marker at one checkpoint to being independently derived and result in a catastrophic unresolved polytomy.
Steady retention maintains a uniform gap size between retained differentiae that grows linearly with generations elapsed, meaning that very recent history is sparsely covered, if at all.
The consequences of this deficiency can be seen in Figure \ref{fig:recency-structure}, which compares the density of reconstructed nodes to ground truth.
Reconstructions from steady policy are entirely missing branching events over the prefatory hundred or so generations.
Figures \clabel{fig:bit-vs-byte-summary-byte-outcomes,fig:bit-vs-byte-summary-bit-outcomes} shows reconstruction outcomes resulting under this inner node loss affect reconstruction outcome.
Under steady policy, very high levels of unresolved reconstruction occur in scenarios with low phylogenetic richness, particularly for recent branching events.
Example reconstructions exhibiting catastrophic comb polytomies characteristic of steady reconstruction of low-richness phylogenies can be seen in \ref{fig:examplepanel}.

\subsection{Bit vs. Byte Differentia Width} \label{sec:bit-vs-byte}

mechanism of error: you have a lineage that diverges three ways between checkpoints (maybe make a figure of this); the two LEAST related happen to collide at the next checkpoint --- therefore, it looks like they're most related even though they're not.

sidebar: we can use differentia size to trade off between error and reconstruction accuracy; using 1 byte differentia essentially drives error to zero (but introduces uncertainty i.e., more polytomies in reconstruction)

\subsection{Differentia Width} \label{sec:bit-vs-byte}

\input{fig/bit-vs-byte-summary}

\subsection{Does it Scale?} \label{sec:scaling}
\input{fig/scaling-summary}


\begin{itemize}
    \item show reconstruction error and inner node count vs. pop size (error does not increase with pop size at fixed sample size)
    \item show reconst error/inner node count vs. time (error does not increase with time at fixed sample size)
    \item show reconst error/inner node count vs. sample size (error increases with sample size)
    \item show reconst error/inner node count vs. bit size at large sample (we can reduce the error for large sample size by increasing bit size)
\end{itemize}

Note that inner node loss will likely be less of an issue for asynchronous generations on account of leaves being generationally dispersed.
